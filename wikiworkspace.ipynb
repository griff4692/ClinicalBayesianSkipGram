{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob,os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import sys\n",
    "from gensim.corpora import WikiCorpus\n",
    "from multiprocessing import Pool\n",
    "import json\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "import torch\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    PAD_TOKEN = '<pad>'\n",
    "\n",
    "    def __init__(self):\n",
    "        self.w2i = {}\n",
    "        self.i2w = []\n",
    "        self.support = []\n",
    "        self.add_token(Vocab.PAD_TOKEN)\n",
    "        self.cached_neg_sample_prob = None\n",
    "\n",
    "    def pad_id(self):\n",
    "        return self.get_id(Vocab.PAD_TOKEN)\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        for tidx, token in enumerate(tokens):\n",
    "            self.add_token(token)\n",
    "\n",
    "    def add_token(self, token, token_support=1):\n",
    "        if token not in self.w2i:\n",
    "            self.w2i[token] = len(self.i2w)\n",
    "            self.i2w.append(token)\n",
    "            self.support.append(0)\n",
    "        self.support[self.get_id(token)] += token_support\n",
    "\n",
    "    def neg_sample(self, size=None):\n",
    "        if self.cached_neg_sample_prob is None:\n",
    "            support = np.array(self.support)\n",
    "            support_raised = np.power(support, 0.75)\n",
    "            support_raised[0] = 0.0  # Never select padding idx\n",
    "            self.cached_neg_sample_prob = support_raised / support_raised.sum()\n",
    "        return np.random.choice(np.arange(self.size()), size=size, p=self.cached_neg_sample_prob)\n",
    "\n",
    "    def get_id(self, token):\n",
    "        if token in self.w2i:\n",
    "            return self.w2i[token]\n",
    "        return -1\n",
    "\n",
    "    def id_count(self, id):\n",
    "        return self.support[id]\n",
    "\n",
    "    def token_count(self, token):\n",
    "        return self.id_count(self.get_id(token))\n",
    "\n",
    "    def get_ids(self, tokens):\n",
    "        return list(map(self.get_id, tokens))\n",
    "\n",
    "    def get_token(self, id):\n",
    "        return self.i2w[id]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class extract_data:\n",
    "\n",
    "    def __init__(self,data_path,path):\n",
    "        self.X = []\n",
    "        self.vocab = []\n",
    "        self.tokens = []\n",
    "        self.keep_prob = []\n",
    "        self.subsampled_ids = []\n",
    "        self.tokenized_subsampled_data = []\n",
    "        self.data_path = data_path\n",
    "        self.path = path\n",
    "        \n",
    "    def make_corpus(self,in_f, out_f):\n",
    "        print(\"Loading data\")\n",
    "        wiki = WikiCorpus(in_f)\n",
    "        print(\"Processing...\")\n",
    "        i = 1\n",
    "        for document in wiki.get_texts():\n",
    "            out_f_ = ''.join([out_f,'\\\\wiki_en_',str(i),\".txt\"])\n",
    "            output = open(out_f_, 'w',encoding='utf-8')\n",
    "            output.write(' '.join(document))\n",
    "            output.close()\n",
    "            i+=1\n",
    "            if i%10000 == 0:  \n",
    "                print(i,\"documents procecessed\")\n",
    "        print('Processing complete!')\n",
    "\n",
    "    def read_data(self,keep_prob = 0.25):\n",
    "            X = []\n",
    "            print(\"Reading txt data\")\n",
    "            os.chdir(self.data_path)\n",
    "            N = len(glob.glob(\"*.txt\"))\n",
    "            #choice = np.random.choice(N, sample_size, replace=False)+1\n",
    "            for i in tqdm(range(1,N+1)):\n",
    "                if np.random.binomial(1, keep_prob):\n",
    "                    text = ''.join([self.data_path,\"\\\\wiki_en_\",str(i),\".txt\"])\n",
    "                    with open(text,'r',encoding='utf-8') as f:\n",
    "                        X.append(''.join(list(f)).lower())\n",
    "                else:\n",
    "                    pass\n",
    "            print(\"Saving sampled wiki data, sample rate:\",keep_prob)\n",
    "            with open(self.path + 'Wiki_tokenized{}.json'.format(''), 'w') as fd:\n",
    "                json.dump(list(zip(range(len(X)),X)), fd)\n",
    "    \n",
    "    def token_counts(self):\n",
    "        print(\"Generating token counts\")\n",
    "        token_cts = defaultdict(int)\n",
    "        doc_id = 0\n",
    "        token_counts_fn = self.path + 'Wiki_tokenized{}.json'.format('')\n",
    "        with open(token_counts_fn, 'r') as fd:\n",
    "            X = json.load(fd)\n",
    "        for doc in X:\n",
    "            for token in doc[1].split():\n",
    "                token_cts[token] += 1\n",
    "                token_cts['__ALL__'] += 1\n",
    "        print(\"Saving token counts\")\n",
    "        with open(self.path + 'Wiki_token_counts{}.json'.format(''), 'w') as fd:\n",
    "            json.dump(token_cts, fd)\n",
    "    \n",
    "    def subsample(self):\n",
    "        print(\"Sub-sampling tokens...\")\n",
    "        tokenized_fp = self.path + 'Wiki_tokenized'\n",
    "        token_counts_fp = self.path + 'Wiki_token_counts'\n",
    "        subsample_param = 0.001\n",
    "        min_token_count = 5\n",
    "        debug_str = ''\n",
    "        tokenized_data_fn = '{}{}.json'.format(tokenized_fp, debug_str)\n",
    "        with open(tokenized_data_fn, 'r') as fd:\n",
    "            tokenized_data = json.load(fd)\n",
    "        token_counts_fn = '{}{}.json'.format(token_counts_fp, debug_str)\n",
    "        with open(token_counts_fn, 'r') as fd:\n",
    "            token_counts = json.load(fd)\n",
    "        N = float(token_counts['__ALL__'])\n",
    "        # And vocabulary with word counts\n",
    "        self.vocab = Vocab()\n",
    "        num_docs = len(tokenized_data)\n",
    "        for doc_idx in tqdm(range(num_docs)):\n",
    "            category, tokenized_doc_str = tokenized_data[doc_idx]\n",
    "            subsampled_doc = []\n",
    "            for token in tokenized_doc_str.split():\n",
    "                wc = token_counts[token]\n",
    "                too_sparse = wc <= min_token_count\n",
    "                if too_sparse:\n",
    "                    continue\n",
    "                frac = wc / N\n",
    "                keep_prob = min((np.sqrt(frac / subsample_param) + 1) * (subsample_param / frac), 1.0)\n",
    "                should_keep = np.random.binomial(1, keep_prob) == 1\n",
    "                if should_keep:\n",
    "                    subsampled_doc.append(token)\n",
    "                    self.vocab.add_token(token, token_support=1)\n",
    "            self.tokenized_subsampled_data.append((category-1, ' '.join(subsampled_doc)))\n",
    "            \n",
    "    def tokens_to_ids(self):\n",
    "        print(\"Converting tokens to ids...\")\n",
    "        for i in tqdm(range(len(self.tokenized_subsampled_data))):\n",
    "            self.subsampled_ids.append(np.asarray([self.vocab.get_id(x) for x in self.tokenized_subsampled_data[i][1].split()]))\n",
    "        self.subsampled_ids = np.asarray(self.subsampled_ids)\n",
    "    \n",
    "    def token_doc_map(self):\n",
    "        print(\"Forming token document matrix... \")\n",
    "        self.token_doc_matrix = np.zeros((self.vocab.size()+1,self.subsampled_ids.shape[0]))\n",
    "        for i in tqdm(range(self.subsampled_ids.shape[0])):\n",
    "            for token_id in self.subsampled_ids[i]:\n",
    "                self.token_doc_matrix[token_id][i] +=1\n",
    "        self.token_doc_matrix = sparse.csr_matrix(self.token_doc_matrix)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading txt data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 100527/100527 [00:26<00:00, 3805.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving sampled wiki data, sample rate: 0.25\n",
      "Generating token counts\n",
      "Saving token counts\n",
      "Sub-sampling tokens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 24903/24903 [00:37<00:00, 672.65it/s]\n",
      "  2%|█▍                                                                          | 466/24903 [00:00<00:05, 4626.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting tokens to ids...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 24903/24903 [00:02<00:00, 9636.94it/s]\n",
      "  1%|▋                                                                           | 206/24903 [00:00<00:12, 2044.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forming token document matrix... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 24903/24903 [00:05<00:00, 4404.58it/s]\n"
     ]
    }
   ],
   "source": [
    "in_f = \"D:\\Latent Meaning Cells\\simplewiki-latest-pages-articles.xml.bz2\"\n",
    "out_f = 'D:\\Latent Meaning Cells\\simplewiki'\n",
    "\n",
    "data_path =  \"D:\\\\Latent Meaning Cells\\\\simplewiki\"\n",
    "path = \"D:\\\\Latent Meaning Cells\\\\\"\n",
    "\n",
    "wiki_data = extract_data(data_path,path)\n",
    "#wiki_data.make_corpus(in_f, out_f)\n",
    "try:\n",
    "    p = Pool(processes=10)\n",
    "    p.apply(wiki_data.read_data())\n",
    "except:\n",
    "    p.close()\n",
    "    wiki_data.token_counts()\n",
    "    wiki_data.subsample()\n",
    "    wiki_data.tokens_to_ids()\n",
    "    wiki_data.token_doc_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_tokens = wiki_data.subsampled_ids\n",
    "vocab = wiki_data.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"D:\\Latent Meaning Cells\\\\vocab.obj\" \n",
    "#file_pi = open(filename, 'wb')\n",
    "#pickle.dump(vocab, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If already extracted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"D:\\Latent Meaning Cells\\sub_tokens.obj\"\n",
    "#filehandler = open(filename, 'rb')\n",
    "#sub_tokens = pickle.load(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"D:\\Latent Meaning Cells\\\\vocab.obj\"\n",
    "#filehandler = open(filename, 'rb')\n",
    "#vocab = pickle.load(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class batcher:\n",
    "\n",
    "    def __init__(self,batch_size,window_size,vocab,sub_tokens):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.window_size = window_size\n",
    "        self.vocab = vocab\n",
    "        self.sub_tokens = sub_tokens\n",
    "        \n",
    "        self.vocab_tokens = np.linspace(1, vocab.size()-1, num=vocab.size()-1).astype(int)\n",
    "        #self.prob = np.power(vocab.support[1:], 0.75)\n",
    "        #self.prob = self.prob/np.sum(self.prob)\n",
    "        \n",
    "    def next(self):\n",
    "        \n",
    "        sub_tokens = self.sub_tokens\n",
    "        batch_size = self.batch_size\n",
    "        window_size = self.window_size\n",
    "        #prob = self.prob\n",
    "        \n",
    "        center_words = np.zeros(batch_size)\n",
    "        vocab_tokens =self.vocab_tokens\n",
    "        num_contexts = np.zeros(batch_size)\n",
    "        \n",
    "        positive_words = np.zeros((batch_size,window_size*2))\n",
    "        negative_words = np.zeros((batch_size,window_size*2))\n",
    "        doc_ids = np.random.choice(len(sub_tokens),batch_size)\n",
    "        len_docs = np.asarray([x.shape[0] for x in sub_tokens[doc_ids]])\n",
    "        center_index = np.asarray([np.random.choice(x) for x in len_docs])\n",
    "        upper_index = np.minimum(center_index+window_size,len_docs-1).astype(int)\n",
    "        lower_index = np.maximum(center_index-window_size,np.zeros(batch_size)).astype(int)\n",
    "            \n",
    "        for i in range(batch_size):\n",
    "        \n",
    "            positive_sub_batch = np.linspace(lower_index[i],upper_index[i], num=upper_index[i]-lower_index[i]+1)\n",
    "            positive_sub_batch = positive_sub_batch[positive_sub_batch != center_index[i]].astype(int)\n",
    "            \n",
    "            num_contexts[i] = positive_sub_batch.shape[0]\n",
    "            \n",
    "            document = sub_tokens[doc_ids[i]]\n",
    "            positive_sub_batch = np.asarray([document[x] for x in positive_sub_batch]).astype(int)\n",
    "            positive_words[i,:positive_sub_batch.shape[0]] = positive_sub_batch\n",
    "\n",
    "            center_words[i] = document[center_index[i]]\n",
    "            \n",
    "            #negative_words_ = vocab_tokens[~np.isin(vocab_tokens, positive_sub_batch)]\n",
    "            #negative_sampling_probability = prob[~np.isin(vocab_tokens, positive_sub_batch)]\n",
    "            #negative_sampling_probability = negative_sampling_probability/np.sum(negative_sampling_probability)\n",
    "            #negative_words[i] = np.random.choice(negative_words_, window_size*2, p=negative_sampling_probability).astype(int)\n",
    "            \n",
    "            negative_words[i] = vocab.neg_sample(window_size*2)\n",
    "            \n",
    "        return doc_ids.astype(int), center_words.astype(int), positive_words.astype(int), negative_words.astype(int),num_contexts.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_2D(target_size, num_contexts):\n",
    "    mask = torch.BoolTensor(target_size)\n",
    "    mask.fill_(0)\n",
    "    for batch_idx, num_c in enumerate(num_contexts):\n",
    "        if num_c < target_size[1]:\n",
    "            mask[batch_idx, num_c:] = 1\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, device,encoder_input_dim,encoder_hidden_dim,latent_dim, token_vocab_size, section_vocab_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.device = device\n",
    "        self.encoder = Encoder(encoder_input_dim,encoder_hidden_dim,latent_dim, token_vocab_size, section_vocab_size)\n",
    "        self.margin = 1.0\n",
    "\n",
    "    def forward(self, center_ids, section_ids, context_ids, neg_context_ids,num_context_ids):\n",
    "        \"\"\"\n",
    "        :param center_ids: batch_size\n",
    "        :param section_ids: batch_size\n",
    "        :param context_ids: batch_size, 2 * context_window\n",
    "        :param neg_context_ids: batch_size, 2 * context_window\n",
    "        :param num_contexts: batch_size (how many context words for each center id - necessary for masking padding)\n",
    "        :return: cost components: KL-Divergence (q(z|w,c) || p(z|w)) and max margin (reconstruction error)\n",
    "        \"\"\"\n",
    "        # Mask padded context ids\n",
    "        batch_size, num_context_ids = context_ids.size()\n",
    "        mask_size = torch.Size([batch_size, num_context_ids])\n",
    "        mask = mask_2D(mask_size, num_contexts).to(self.device)\n",
    "\n",
    "        # Compute center words\n",
    "        mu_center, sigma_center = self.encoder(center_ids, section_ids)\n",
    "        mu_center_tiled = mu_center.unsqueeze(1).repeat(1, num_context_ids, 1)\n",
    "        sigma_center_tiled = sigma_center.unsqueeze(1).repeat(1, num_context_ids, 1)\n",
    "        mu_center_flat = mu_center_tiled.view(batch_size * num_context_ids, -1)\n",
    "        sigma_center_flat = sigma_center_tiled.view(batch_size * num_context_ids, -1)\n",
    "\n",
    "        # Tile section ids for positive and negative samples\n",
    "        section_ids_tiled = section_ids.unsqueeze(-1).repeat(1, num_context_ids)\n",
    "\n",
    "        # Compute positive and negative encoded samples\n",
    "        mu_pos_context, sigma_pos_context = self.encoder(context_ids, section_ids_tiled)\n",
    "        mu_neg_context, sigma_neg_context = self.encoder(neg_context_ids, section_ids_tiled)\n",
    "\n",
    "        # Flatten positive context\n",
    "        mu_pos_context_flat = mu_pos_context.view(batch_size * num_context_ids, -1)\n",
    "        sigma_pos_context_flat = sigma_pos_context.view(batch_size * num_context_ids, -1)\n",
    "\n",
    "        # Flatten negative context\n",
    "        mu_neg_context_flat = mu_neg_context.view(batch_size * num_context_ids, -1)\n",
    "        sigma_neg_context_flat = sigma_neg_context.view(batch_size * num_context_ids, -1)\n",
    "\n",
    "        # Compute KL-divergence between center words and negative and reshape\n",
    "        kl_pos_flat = compute_kl(mu_center_flat, sigma_center_flat, mu_pos_context_flat, sigma_pos_context_flat)\n",
    "        kl_neg_flat = compute_kl(mu_center_flat, sigma_center_flat, mu_neg_context_flat, sigma_neg_context_flat)\n",
    "        kl_pos = kl_pos_flat.view(batch_size, num_context_ids)\n",
    "        kl_neg = kl_neg_flat.view(batch_size, num_context_ids)\n",
    "\n",
    "        hinge_loss = (kl_pos - kl_neg + self.margin).clamp_min_(0)\n",
    "        hinge_loss = hinge_loss.masked_fill(mask, 0)\n",
    "        hinge_loss = hinge_loss.sum(1)\n",
    "        return hinge_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data\n",
    " \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_input_dim,encoder_hidden_dim,latent_dim, token_vocab_size, section_vocab_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.f = nn.Linear(encoder_input_dim * 2, encoder_hidden_dim, bias=True)\n",
    "        self.u = nn.Linear(encoder_hidden_dim, latent_dim, bias=True)\n",
    "        self.v = nn.Linear(encoder_hidden_dim, 1, bias=True)\n",
    "    \n",
    "        self.token_embeddings = nn.Embedding(token_vocab_size, encoder_input_dim, padding_idx=0)\n",
    "        #self.token_embeddings.weight.data.uniform_(-1, 1)\n",
    "        self.section_embeddings = nn.Embedding(section_vocab_size, encoder_input_dim)\n",
    "        #self.section_embeddings.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def forward(self, center_ids, section_ids):\n",
    "        \"\"\"\n",
    "        :param center_ids: LongTensor of batch_size\n",
    "        :param context_ids: LongTensor of batch_size\n",
    "        :param mask: BoolTensor of batch_size x 2 * context_window (which context_ids are just the padding idx)\n",
    "        :return: mu (batch_size, latent_dim), logvar (batch_size, 1)\n",
    "        \"\"\"\n",
    "        center_embedding = self.token_embeddings(center_ids)\n",
    "        section_embedding = self.section_embeddings(section_ids)\n",
    "            \n",
    "        merged_embeds = self.dropout(torch.cat([center_embedding, section_embedding], dim=-1))\n",
    "            \n",
    "        h = self.dropout(F.relu(self.f(merged_embeds)))\n",
    "        var_clamped = self.v(h).exp().clamp_min(1.0)\n",
    "        return self.u(h), var_clamped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl(mu_a, sigma_a, mu_b, sigma_b, device=None):\n",
    "    \"\"\"\n",
    "    :param mu_a: mean vector of batch_size x dim\n",
    "    :param sigma_a: standard deviation of batch_size x {1, dim}\n",
    "    :param mu_b: mean vector of batch_size x dim\n",
    "    :param sigma_b: standard deviation of batch_size x {1, dim}\n",
    "    :return: computes KL-Divergence between 2 diagonal Gaussian (a||b)\n",
    "    \"\"\"\n",
    "    var_dim = sigma_a.size()[-1]\n",
    "    assert sigma_b.size()[-1] == var_dim\n",
    "    if var_dim == 1:\n",
    "        return kl_spher(mu_a, sigma_a, mu_b, sigma_b)\n",
    "    return kl_diag(mu_a, sigma_a, mu_b, sigma_b, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_spher(mu_a, sigma_a, mu_b, sigma_b):\n",
    "    \"\"\"\n",
    "    :param mu_a: mean vector of batch_size x dim\n",
    "    :param sigma_a: standard deviation of batch_size x 1\n",
    "    :param mu_b: mean vector of batch_size x dim\n",
    "    :param sigma_b: standard deviation of batch_size x 1\n",
    "    :return: computes KL-Divergence between 2 spherical Gaussian (a||b)\n",
    "    \"\"\"\n",
    "    d = mu_a.shape[1]\n",
    "    sigma_p_inv = 1.0 / sigma_b  # because diagonal\n",
    "    tra = d * sigma_a * sigma_p_inv\n",
    "    quadr = sigma_p_inv * torch.pow(mu_b - mu_a, 2).sum(1, keepdim=True)\n",
    "    log_det = - d * torch.log(sigma_a * sigma_p_inv)\n",
    "    res = 0.5 * (tra + quadr - d + log_det)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\"\n",
    "encoder_input_dim = 100\n",
    "encoder_hidden_dim = 64\n",
    "latent_dim = 100\n",
    "\n",
    "token_vocab_size = vocab.size()\n",
    "section_vocab_size = sub_tokens.shape[0]\n",
    "\n",
    "model = VAE(device,encoder_input_dim,encoder_hidden_dim,latent_dim, token_vocab_size, section_vocab_size).to(device)\n",
    "\n",
    "trainable_params = filter(lambda x: x.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(trainable_params, lr=0.1)\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/44 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 44/44 [00:27<00:00,  1.58it/s]\n",
      "  0%|                                                                                           | 0/44 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=1. Loss=8.789356231689453.\n",
      "Starting Epoch=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 44/44 [00:27<00:00,  1.58it/s]\n",
      "  0%|                                                                                           | 0/44 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=2. Loss=8.2186279296875.\n",
      "Starting Epoch=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 44/44 [00:28<00:00,  1.57it/s]\n",
      "  0%|                                                                                           | 0/44 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=3. Loss=8.011993408203125.\n",
      "Starting Epoch=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 44/44 [00:28<00:00,  1.53it/s]\n",
      "  0%|                                                                                           | 0/44 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=4. Loss=7.812597274780273.\n",
      "Starting Epoch=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|████████████████████████████████████████████████████████████████████████████▍     | 41/44 [00:27<00:01,  1.52it/s]"
     ]
    }
   ],
   "source": [
    "window_size = 5\n",
    "batch_size = 1024\n",
    "num_epoch = 70\n",
    "num_contexts = batch_size\n",
    "generator = batcher(batch_size,window_size,vocab,sub_tokens)\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "    sleep(0.1)  # Make sure logging is synchronous with tqdm progress bar\n",
    "    print('Starting Epoch={}'.format(epoch))\n",
    "    generator = batcher(batch_size,window_size,vocab,sub_tokens)\n",
    "    num_batches = batch_size\n",
    "    epoch_loss = 0.0\n",
    "    for _ in tqdm(range(int(vocab.size()/num_batches)), position=0, leave=True):\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        section_ids,center_ids, context_ids, neg_ids,num_contexts = generator.next()\n",
    "        \n",
    "        center_ids_tens = torch.LongTensor(center_ids).to(device)\n",
    "        context_ids_tens = torch.LongTensor(context_ids).to(device)\n",
    "        section_ids_tens = torch.LongTensor(section_ids).to(device)\n",
    "        neg_ids_tens = torch.LongTensor(neg_ids).to(device)\n",
    "\n",
    "        loss = model(center_ids_tens, section_ids_tens, context_ids_tens, neg_ids_tens,num_contexts)\n",
    "        loss.backward()  # backpropagate loss\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    sleep(0.1)\n",
    "    print('Epoch={}. Loss={}.'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = np.zeros(vocab.size())\n",
    "month = model.encoder.token_embeddings(torch.Tensor(np.asarray([293])).long().to(\"cuda\")).data.cpu().numpy()\n",
    "for i in range(vocab.size()):\n",
    "    diff = model.encoder.token_embeddings(torch.Tensor(np.asarray([i])).long().to(\"cuda\")).data.to(\"cpu\").numpy() - month\n",
    "    dist = np.sum(np.dot(diff,diff.T))\n",
    "    dists[i] = dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meaning distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.token_embeddings(torch.Tensor(np.asarray([0])).long().to(\"cuda\")).data.cpu().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
